{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ff949",
   "metadata": {},
   "outputs": [],
   "source": [
    "global ROOT_DIR\n",
    "ROOT_DIR = ''\n",
    "SAVE_DIR = f'{ROOT_DIR}/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "sys.path.append(f'{ROOT_DIR}/code/wasserstein')\n",
    "import WassersteinOTCost as wot\n",
    "import importlib\n",
    "importlib.reload(wot)\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "from scipy.stats import bootstrap\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242b59d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, cost):\n",
    "    if dataset == 'Synthetic':\n",
    "        cost = \"{:.2f}\".format(cost)\n",
    "        d1 = pd.read_csv(f'{ROOT_DIR}/data/{dataset}/data_1_{cost}.csv',sep = ' ', names = [i for i in range(13)])\n",
    "        d2 = pd.read_csv(f'{ROOT_DIR}/data/{dataset}/data_2_{cost}.csv', sep = ' ', names = [i for i in range(13)])\n",
    "        X1, y1 = d1.iloc[:,:-1].values, d1.iloc[:,-1].values\n",
    "        X2, y2 = d2.iloc[:,:-1].values, d2.iloc[:,-1].values\n",
    "    \n",
    "    elif dataset == 'Credit':\n",
    "        cost = \"{:.2f}\".format(cost)\n",
    "        d1 = pd.read_csv(f'{ROOT_DIR}/data/{dataset}/data_1_{cost}.csv',sep = ' ', names = [i for i in range(29)])\n",
    "        d2 = pd.read_csv(f'{ROOT_DIR}/data/{dataset}/data_2_{cost}.csv', sep = ' ', names = [i for i in range(29)])\n",
    "        d1 = d1.sample(n=500)\n",
    "        d2 = d2.sample(n=500)\n",
    "        X1, y1 = d1.iloc[:,:-1].values, d1.iloc[:,-1].values\n",
    "        X2, y2 = d2.iloc[:,:-1].values, d2.iloc[:,-1].values\n",
    "    \n",
    "    elif dataset == 'Weather':\n",
    "        X1,  y1,  X2, y2 = load_data_weather(cost)\n",
    "\n",
    "    elif dataset == 'EMNIST':\n",
    "        cost = \"{:.2f}\".format(cost)\n",
    "        d1 = np.load(f'{ROOT_DIR}/data/{dataset}/data_1_{cost}.npz')\n",
    "        d2 = np.load(f'{ROOT_DIR}/data/{dataset}/data_2_{cost}.npz')\n",
    "\n",
    "        X1, y1 = d1['data'], d1['labels']\n",
    "        idx = np.random.choice(np.arange(X1.shape[0]), 500, replace=False)\n",
    "        X1, y1 = X1[idx], y1[idx]\n",
    "        X1 = X1.reshape(28*28,-1)\n",
    "\n",
    "        X2, y2 = d2['data'], d2['labels']\n",
    "        idx = np.random.choice(np.arange(X2.shape[0]), 500, replace=False)\n",
    "        X2, y2 = X2[idx], y2[idx]\n",
    "        X2 = X2.reshape(28*28,-1)\n",
    "\n",
    "    elif dataset == 'CIFAR':\n",
    "        cost = \"{:.2f}\".format(cost)\n",
    "        X1, X2, y1, y2 = load_cifar_embeddings(cost)\n",
    "\n",
    "    elif dataset == 'IXITiny':\n",
    "        X1 = load_data_ixitiny(cost[0])\n",
    "        X2 = load_data_ixitiny(cost[1])\n",
    "        return {\"1\":X1, \"2\":X2}\n",
    "    \n",
    "    elif dataset == 'ISIC':\n",
    "        X1, y1 = load_data_isic(cost[0])\n",
    "        X2, y2 = load_data_isic(cost[1])\n",
    "        \n",
    "    return {\"1\":X1, \"2\":X2}, {\"1\":y1, \"2\":y2}\n",
    "\n",
    "def load_data_weather(costs):\n",
    "     weather_df = load_all_weather()\n",
    "     return dictionaryCreator(weather_df, costs, n = 5000)\n",
    "\n",
    "def load_all_weather():\n",
    "    DATA_DIR = f'{ROOT_DIR}/data/Weather'\n",
    "    ##load dataset\n",
    "    df = pd.read_csv(f'{DATA_DIR}/shifts_canonical_train.csv', nrows = 20000)\n",
    "    df[((df['climate'] == 'tropical') & (df['fact_temperature'] > 25)) | \n",
    "        ((df['climate'] == 'mild temperate') & ((df['fact_temperature'] > 10) & (df['fact_temperature'] < 25))) |\n",
    "        (df['climate'] == 'dry') & ((df['fact_temperature'] > 5) & (df['fact_temperature'] < 25))]\n",
    "    df_snow = pd.read_csv(f'{DATA_DIR}/shifts_canonical_eval_out.csv', nrows = 20000)\n",
    "    df_snow = df_snow[df_snow['fact_temperature'] < 10]\n",
    "    df = pd.concat([df, df_snow])\n",
    "    df.dropna(inplace = True)\n",
    "    return df\n",
    "\n",
    "def extractData(df, climate, n=5000):\n",
    "    df = df[df['climate'].isin(climate)]\n",
    "    ind = np.random.choice(df.shape[0], n)\n",
    "    X = df.iloc[ind, 6:]\n",
    "    y = df.iloc[ind, 5]\n",
    "    return X.values, y.values\n",
    "\n",
    "def dictionaryCreator(df, climates, n = 5000):\n",
    "    ##wrangle to dictionary for OT cost calculation\n",
    "    X1, y1 = extractData(df, climates[0],n = n)\n",
    "    scaler = StandardScaler()\n",
    "    X1_normalized = scaler.fit_transform(X1)   \n",
    "    X2, y2 = extractData(df, climates[1],n = n)\n",
    "    X2_normalized = scaler.transform(X2)  \n",
    "   \n",
    "    return X1_normalized,  y1,  X2_normalized, y2\n",
    "\n",
    "\n",
    "class EmbeddedImagesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, embedding, label, coarse_label = self.data[idx]\n",
    "        return image, embedding, label, coarse_label\n",
    "    \n",
    "def extract_by_labels(dataset, target_labels, label_type = 'fine'):\n",
    "    extracted_data = []\n",
    "    for image, embedding, fine_label, coarse_label in dataset:\n",
    "        if label_type == 'fine':\n",
    "            label = fine_label\n",
    "        else:\n",
    "            label = coarse_label\n",
    "        if label in target_labels:\n",
    "            extracted_data.append((image, embedding, fine_label, coarse_label))\n",
    "    return EmbeddedImagesDataset(extracted_data)\n",
    "\n",
    "def get_datasets(dataset, labels_extract, label_type = 'fine'):\n",
    "    d1 = extract_by_labels(dataset, labels_extract[0], label_type)\n",
    "    d2 = extract_by_labels(dataset, labels_extract[1], label_type)\n",
    "    return d1, d2\n",
    "\n",
    "def sampler(dataset, num_samples):\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    sampled_data = [dataset[i] for i in indices]\n",
    "    embs = np.array([entry[1] for entry in sampled_data])\n",
    "    label = np.array([entry[2] for entry in sampled_data])\n",
    "    return embs, label\n",
    "\n",
    "def load_cifar_embeddings(cost):\n",
    "    labels = {'0.08': [[x for x in range(10)], [x for x in range(10)]],\n",
    "            '0.21': [[11,98,29,73, 78, 49, 97, 51, 55, 92], [11,98,29,73, 78, 49, 42, 83, 72, 82]],\n",
    "            '0.30':[[11,50,78,1,92, 78, 49, 97, 55, 16, 14], [11, 36, 29, 73, 82, 78, 49, 42, 12, 23, 51]],\n",
    "            '0.38':[[11,50,78,8,92,2,49,98,89,3], [17, 36, 30, 73, 83,28, 34, 42, 10, 20]]}\n",
    "            \n",
    "    labels_extract = labels[cost]\n",
    "    with open(f'{ROOT_DIR}/data/CIFAR/cifar_{1000}_emb.pkl', 'rb') as f:\n",
    "        data= pickle.load(f)\n",
    "    d1, d2 = get_datasets(data, labels_extract)\n",
    "    num_samples = 500\n",
    "    X1, y1 =  sampler(d1, num_samples)\n",
    "    X2, y2 =  sampler(d2, num_samples)\n",
    "    return X1, X2, y1, y2\n",
    "\n",
    "def load_data_ixitiny(site):\n",
    "    files = os.listdir(f'{ROOT_DIR}/data/IXITiny/embedding')\n",
    "    files_site = [file for file in files if site in file]\n",
    "    embeddings = []\n",
    "    for file in files_site:\n",
    "        emb = np.load(f'{ROOT_DIR}/data/IXITiny/embedding/{file}')\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def load_data_isic(site):\n",
    "    files = pd.read_csv(f'{ROOT_DIR}/data/ISIC/site_{site}_files_used.csv')\n",
    "    files = files.sample(500)\n",
    "    label_counts = files['label'].value_counts()\n",
    "    selected_labels = label_counts[label_counts > 30].index\n",
    "    filtered_files = files[files['label'].isin(selected_labels)]\n",
    "    files_used  = list(filtered_files['image'].values)\n",
    "    labels_used = list(filtered_files['label'].values)\n",
    "    embeddings = []\n",
    "    for file in files_used:\n",
    "        emb = np.load(f'{ROOT_DIR}/data/ISIC/embedding/center_{site}_{file}.npy')\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings, np.array(labels_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033cca50",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(DATASET, wasserstein_costs):\n",
    "    with open(f'{ROOT_DIR}/results/{DATASET}/{DATASET}_scores_full.pkl', 'rb') as f :\n",
    "        results = pickle.load(f)\n",
    "        new_results = dict(zip(wasserstein_costs, results.values()))\n",
    "    return new_results\n",
    "    \n",
    "def bootstrap_ci(data, alpha=0.95):\n",
    "    estimates = {}\n",
    "    for c in data:\n",
    "        estimates[c]={}\n",
    "        for arch in data[c]:\n",
    "            res = data[c][arch]\n",
    "            median = np.mean(res)\n",
    "            bs_reps = bootstrap(np.array(res).reshape(1,-1), statistic=np.mean, n_resamples=1000)\n",
    "            ci = bs_reps.confidence_interval[0:2]\n",
    "            estimates[c][arch] = (median, ci[0], ci[1])\n",
    "    return estimates\n",
    "\n",
    "def bootstrap_samples(data, n_iterations):\n",
    "    n = len(data)\n",
    "    indices = np.random.randint(0, n, (n_iterations, n))\n",
    "    return data[indices]\n",
    "\n",
    "def get_difference_estimates(results):\n",
    "    results_diff = {}\n",
    "    n_iterations = 1000\n",
    "    for cost in results:\n",
    "        results_diff[cost] = {}\n",
    "        for architecture in list(results[cost].keys())[1:]:\n",
    "            single = np.array(results[cost]['single'])\n",
    "            other = np.array(results[cost][architecture])\n",
    "            bs_single_samples = bootstrap_samples(single, n_iterations)\n",
    "            bs_other_samples = bootstrap_samples(other, n_iterations)\n",
    "            bs_single_means = np.median(bs_single_samples, axis=1)\n",
    "            bs_other_means = np.median(bs_other_samples, axis=1)\n",
    "            bs_diff = bs_other_means - bs_single_means \n",
    "            mean_single = np.median(single)\n",
    "            median_diff = 100 * np.percentile(bs_diff, 50) / mean_single\n",
    "            lower_ci_diff = 100 * np.percentile(bs_diff, 5) / mean_single\n",
    "            upper_ci_diff = 100 * np.percentile(bs_diff, 95) / mean_single\n",
    "            results_diff[cost][architecture] = (np.round(median_diff, 3), np.round(lower_ci_diff, 3), np.round(upper_ci_diff, 3))\n",
    "    return pd.DataFrame.from_dict(results_diff, orient='index')\n",
    "\n",
    "def print_med_results(DATASET):\n",
    "    results = load_results(DATASET)\n",
    "    for arch in ['single', 'joint', 'federated', 'pfedme', 'ditto']:\n",
    "        print(arch)\n",
    "        for c in results:\n",
    "            print(np.median(results[c][arch]))\n",
    "        print('----')\n",
    "\n",
    "def grapher(results, DATASET, metric, costs, save = True):\n",
    "    results_long = results.reset_index().rename(columns={'index': 'cost'}).melt(id_vars=['cost'], var_name='architecture')\n",
    "    results_long[['median_diff', 'lower_ci_diff', 'upper_ci_diff']] = pd.DataFrame(results_long['value'].tolist(), index=results_long.index)\n",
    "    results_long.drop(columns=['value'], inplace=True)\n",
    "    results_long\n",
    "    plt.figure()\n",
    "    for architecture in results_long['architecture'].unique():\n",
    "        subset = results_long[results_long['architecture'] == architecture]\n",
    "        sns.lineplot(x='cost', y='median_diff', marker = 'o', data=subset, label=architecture.capitalize())\n",
    "        if DATASET not in ['IXITiny', 'ISIC']:\n",
    "            plt.fill_between(x=subset['cost'], y1=subset['lower_ci_diff'], y2=subset['upper_ci_diff'], alpha=0.2)\n",
    "    plt.axhline(y=0, color='black', linestyle = '--', alpha = 0.5, label = 'Baseline')\n",
    "    plt.xlabel('Dataset Cost', fontsize = 14)\n",
    "    plt.ylabel(f'% Change in {metric}', fontsize = 14)\n",
    "    plt.legend(fontsize = 14)\n",
    "    plt.xticks(fontsize = 12)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    if save == True:\n",
    "        plt.savefig(f'{SAVE_DIR}/wasserstein/wasserstein_{DATASET}_scores_change.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def process_results(DATASET, wasserstein_costs):\n",
    "    dataset_dict = {'Synthetic':['F1', '0.03-0.5'],\n",
    "                'Credit': ['F1', '0.12-0.40'],\n",
    "                'Weather': ['R2', '0.11-0.48'],\n",
    "                'EMNIST': ['Accuracy', '0.11-0.39'],\n",
    "                'CIFAR': ['Accuracy', '0.08-0.38'],\n",
    "                'ISIC': ['Balanced Accuracy', '0.06-0.30'],\n",
    "                'IXITiny': ['DICE', '0.08-0.30']}\n",
    "\n",
    "    results = load_results(DATASET, wasserstein_costs)\n",
    "    results_estimates = bootstrap_ci(results, alpha=0.95)\n",
    "    results_diff_estimates = get_difference_estimates(results)\n",
    "    metric, costs = dataset_dict[DATASET]\n",
    "    save = True\n",
    "    grapher(results_diff_estimates, DATASET, metric, costs, save)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb087035",
   "metadata": {},
   "source": [
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba953e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wot)\n",
    "dataset = 'Synthetic'\n",
    "costs = [0.03, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "results_synthetic = []\n",
    "for cost in costs:\n",
    "    data, label= load_data(dataset, cost)\n",
    "    ## calculate cost\n",
    "    synthetic_wot = wot.OTCost(dataset, data, label)\n",
    "    results_synthetic.append(synthetic_wot.calculate_ot_cost())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([costs, results_synthetic]).T\n",
    "df.columns = ['Ours', 'Wasserstein']\n",
    "path = f'{ROOT_DIR}/results/wasserstein'\n",
    "df.to_csv(f'{path}/Wasserstein_{dataset}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7eea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'Synthetic'\n",
    "process_results(DATASET, results_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9e7db",
   "metadata": {},
   "source": [
    "## Credit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7908d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wot)\n",
    "costs = [0.12, 0.23, 0.3, 0.4]\n",
    "dataset = 'Credit'\n",
    "results_credit = []\n",
    "for cost in costs:\n",
    "    data, label= load_data(dataset, cost)\n",
    "    ## calculate cost\n",
    "    credit_wot = wot.OTCost(dataset, data, label)\n",
    "    results_credit.append(credit_wot.calculate_ot_cost())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([costs, results_credit]).T\n",
    "df.columns = ['Ours', 'Wasserstein']\n",
    "path = f'{ROOT_DIR}/results/wasserstein'\n",
    "df.to_csv(f'{path}/Wasserstein_{dataset}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa0024",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'Credit'\n",
    "process_results(DATASET, results_credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e78db",
   "metadata": {},
   "source": [
    "## Weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wot)\n",
    "costs = [0.11, 0.19, 0.3, 0.4, 0.48]\n",
    "climates = [[['tropical', 'mild temperate'],['tropical', 'mild temperate']],\n",
    "            [['tropical', 'mild temperate'], ['dry', 'mild temperate']],\n",
    "            [['tropical', 'mild temperate'], ['dry']],\n",
    "            [['tropical', 'mild temperate'], ['snow', 'dry']],\n",
    "            [['tropical', 'mild temperate'], ['snow']]]\n",
    "dataset = 'Weather'\n",
    "results_weather = []\n",
    "for climate in climates:\n",
    "    data, label = load_data(dataset, climate)\n",
    "    ## calculate cost\n",
    "    weather_wot = wot.OTCost(dataset, data, label)\n",
    "    results_weather.append(weather_wot.calculate_ot_cost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_weather_s = sorted(results_weather)\n",
    "df = pd.DataFrame([costs, results_weather_s]).T\n",
    "df.columns = ['Ours', 'Wasserstein']\n",
    "path = f'{ROOT_DIR}/results/wasserstein'\n",
    "df.to_csv(f'{path}/Wasserstein_{dataset}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'Weather'\n",
    "process_results(DATASET, results_weather_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b4a8a",
   "metadata": {},
   "source": [
    "## EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wot)\n",
    "costs = [0.11, 0.19, 0.25, 0.34, 0.39]\n",
    "dataset = 'EMNIST'\n",
    "results_emnist = []\n",
    "for cost in costs:\n",
    "    data, label= load_data(dataset, cost)\n",
    "    ## calculate cost\n",
    "    emnist_wot = wot.OTCost(dataset, data, label)\n",
    "    results_emnist.append(emnist_wot.calculate_ot_cost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([costs, results_emnist]).T\n",
    "df.columns = ['Ours', 'Wasserstein']\n",
    "path = f'{ROOT_DIR}/results/wasserstein'\n",
    "df.to_csv(f'{path}/Wasserstein_{dataset}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'EMNIST'\n",
    "process_results(DATASET, results_emnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce2de61",
   "metadata": {},
   "source": [
    "## CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dce4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wot)\n",
    "costs = [0.08, 0.21, 0.3, 0.38]\n",
    "dataset = 'CIFAR'\n",
    "results_cifar = []\n",
    "for cost in costs:\n",
    "    data, label= load_data(dataset, cost)\n",
    "    ## calculate cost\n",
    "    cifar_wot = wot.OTCost(dataset, data, label)\n",
    "    results_cifar.append(cifar_wot.calculate_ot_cost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cifar= sorted(results_cifar) #issue with ordering\n",
    "df = pd.DataFrame([costs, results_cifar]).T\n",
    "df.columns = ['Ours', 'Wasserstein']\n",
    "path = f'{ROOT_DIR}/results/wasserstein'\n",
    "df.to_csv(f'{path}/Wasserstein_{dataset}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87386511",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR'\n",
    "process_results(DATASET, results_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11772c9",
   "metadata": {},
   "source": [
    "## IXITiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wot)\n",
    "costs = [0.08, 0.28, 0.30]\n",
    "sites =  [['Guys', 'HH'],\n",
    "            ['Guys', 'IOP'],\n",
    "            ['HH', 'IOP']]\n",
    "dataset = 'IXITiny'\n",
    "results_ixitiny = []\n",
    "for site in sites:\n",
    "    data = load_data(dataset, site)\n",
    "    label = {'1': np.ones(data['1'].shape[0]), '2': np.ones(data['2'].shape[0])}\n",
    "    ## calculate cost\n",
    "    ixitiny_wot = wot.OTCost(dataset, data, label)\n",
    "    results_ixitiny.append(ixitiny_wot.calculate_ot_cost())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea480c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([costs, results_ixitiny]).T\n",
    "df.columns = ['Ours', 'Wasserstein']\n",
    "path = f'{ROOT_DIR}/results/wasserstein'\n",
    "df.to_csv(f'{path}/Wasserstein_{dataset}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bf77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'IXITiny'\n",
    "process_results(DATASET, results_ixitiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f37086",
   "metadata": {},
   "source": [
    "## ISIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wot)\n",
    "costs = [0.08, 0.21, 0.30, 0.38]\n",
    "sites = [(2,2), (2,0), (2,3), (2,1), (1,3)]\n",
    "dataset = 'ISIC'\n",
    "results_isic = []\n",
    "for cost, site in zip(costs, sites):\n",
    "    data, label = load_data(dataset, site)\n",
    "    ## calculate cost\n",
    "    isic_wot = wot.OTCost(dataset, data, label)\n",
    "    results_isic.append(isic_wot.calculate_ot_cost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18af274",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_isic_s = sorted(results_isic)\n",
    "df = pd.DataFrame([costs, results_isic_s]).T\n",
    "df.columns = ['Ours', 'Wasserstein']\n",
    "path = f'{ROOT_DIR}/results/wasserstein'\n",
    "df.to_csv(f'{path}/Wasserstein_{dataset}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87924681",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ISIC'\n",
    "process_results(DATASET, results_isic_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
